{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "baHO_m1fkWh_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "from openai import OpenAI\n",
        "\n",
        "# 配置日志\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# API 配置\n",
        "client = OpenAI(\n",
        "    base_url='https://xiaoai.plus/v1',  # 验证是否需要 /chat/completions\n",
        "    api_key='sk-PV4uwhRGKYRoP0z2D9wBzFMFRsPY1NI80kIUwii3p0CMniaN'  # 替换为实际API密钥\n",
        ")\n",
        "\n",
        "# 模拟参数\n",
        "NUM_SIMULATIONS = 1000  # 初始测试，成功后设为1000\n",
        "PRIOR_P = 0.3  # P(theta = theta_H)\n",
        "STATES = {\"theta_L\": 0, \"theta_H\": 1}\n",
        "SIGNALS = [\"s_L\", \"s_H\"]\n",
        "ACTIONS = [0, 1]\n",
        "MAX_CONSECUTIVE_ERRORS = 5\n",
        "\n",
        "# API调用函数\n",
        "def call_llm(prompt, role=\"sender\", max_retries=3):\n",
        "    logging.info(f\"{role} 提示: {prompt}\")\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a precise assistant that outputs only the requested value.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=100,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            content = completion.choices[0].message.content.strip()\n",
        "            logging.info(f\"{role} API响应: {content} (耗时: {time.time() - start_time:.2f}秒)\")\n",
        "            # 解析响应\n",
        "            if role == \"sender\":\n",
        "                content = content.lower()\n",
        "                if \"s_h\" in content:\n",
        "                    return \"s_H\"\n",
        "                if \"s_l\" in content:\n",
        "                    return \"s_L\"\n",
        "                if content not in SIGNALS:\n",
        "                    logging.warning(f\"无效信号: {content}\")\n",
        "                    return \"s_L\"\n",
        "            else:  # receiver\n",
        "                content = content.strip()\n",
        "                if content in [\"0\", \"1\"]:\n",
        "                    return content\n",
        "                logging.warning(f\"无效动作: {content}\")\n",
        "                return \"0\"\n",
        "            return content\n",
        "        except Exception as e:\n",
        "            logging.error(f\"API调用错误 ({role}, 尝试 {attempt+1}): {str(e)}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "# 发送者提示\n",
        "def sender_prompt(state, trial_number, history):\n",
        "    s_L_H_count = history[\"s_L_H_count\"]\n",
        "    s_L_L_count = history[\"s_L_L_count\"]\n",
        "    s_H_H_count = history[\"s_H_H_count\"]\n",
        "    s_H_L_count = history[\"s_H_L_count\"]\n",
        "    s_L_a1_count = history[\"s_L_a1_count\"]\n",
        "    s_H_a1_count = history[\"s_H_a1_count\"]\n",
        "    prompt = f\"\"\"\n",
        "You are the sender in an information design game. You know the state and send a signal to influence the receiver's action. The receiver learns the true state (theta) after each decision and updates their beliefs about your signals. The game is:\n",
        "- State Space: (theta_L = 0, theta_H = 1).\n",
        "- State: current theta = {state}\n",
        "- Signals: s_L(means theta_L) or s_H(means theta_H).\n",
        "- Receiver's actions: a = 0 or 1.\n",
        "- Payoffs:\n",
        "  - Receiver: u_R(a=1, theta_H) = 1, u_R(a=1, theta_L) = -1, u_R(a=0, theta) = 0.\n",
        "  - You: u_S(a=1) = 1, u_S(a=0) = 0.\n",
        "- Prior: P(theta_H) = 0.3.\n",
        "- Past interactions (summary of {trial_number} trials):\n",
        "  - s_L: {s_L_H_count} times theta_H, {s_L_L_count} times theta_L, a=1 {s_L_a1_count} times.\n",
        "  - s_H: {s_H_H_count} times theta_H, {s_H_L_count} times theta_L, a=1 {s_H_a1_count} times.\n",
        "\n",
        "Goal: Maximize a=1 probability with history information, the receiver can infer the true state from history information and your signal(P(theta_H|s_H)).\n",
        "\n",
        "you should be honest in first serveral trials to avoid cheap talk(truthfully report in first serveral trivals).\n",
        "\n",
        "Respond only with the signal: s_L or s_H. Do not include any other text.\n",
        "\n",
        "Signal: ?\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# 接收者提示\n",
        "def receiver_prompt(signal, trial_number, history):\n",
        "    s_L_H_count = history[\"s_L_H_count\"]\n",
        "    s_L_L_count = history[\"s_L_L_count\"]\n",
        "    s_H_H_count = history[\"s_H_H_count\"]\n",
        "    s_H_L_count = history[\"s_H_L_count\"]\n",
        "    s_L_a1_count = history[\"s_L_a1_count\"]\n",
        "    s_H_a1_count = history[\"s_H_a1_count\"]\n",
        "    p_theta_H_s_H = (s_H_H_count+1e-4) / (s_H_H_count + s_H_L_count + 1e-10)\n",
        "    p_theta_H_s_L = s_L_H_count / (s_L_H_count + s_L_L_count + 1e-10)\n",
        "    prompt = f\"\"\"\n",
        "You are the receiver in an information design game. You receive a signal, choose an action, and learn the true state (theta), updating your beliefs. The game is:\n",
        "- State: theta = theta_L (0) or theta_H (1).\n",
        "- Signal space: s_H (means theta_H) and s_L(means theta_L)\n",
        "- Signal you received: s = {signal}.\n",
        "- Actions: a = 0 or 1.\n",
        "- Payoffs:\n",
        "  - You: u_R(a=1, theta_H) = 1, u_R(a=1, theta_L) = -1, u_R(a=0, theta) = 0.\n",
        "  - Sender: u_S(a=1) = 1, u_S(a=0) = 0.\n",
        "- Prior: P(theta_H) = 0.3.\n",
        "- Past interactions (summary of {trial_number} trials):\n",
        "  - s_L: {s_L_H_count + s_L_L_count} times ({s_L_H_count} theta_H, {s_L_L_count} theta_L), a=1 {s_L_a1_count} times.\n",
        "  - s_H: {s_H_H_count + s_H_L_count} times ({s_H_H_count} theta_H, {s_H_L_count} theta_L), a=1 {s_H_a1_count} times.\n",
        "- Estimated beliefs:\n",
        "  - P(theta_H|s_H) = {p_theta_H_s_H:.2f}\n",
        "Goal: Maximize expected payoff, using signal and history(breaking ties with a = 1).\n",
        "\n",
        "Respond only with the action: 0 or 1. Do not include any other text.\n",
        "\n",
        "\n",
        "Action: ?\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# 模拟交互实验\n",
        "def run_simulation():\n",
        "    results = []\n",
        "    history = {\n",
        "        \"s_L_H_count\": 0,  # 种子数据\n",
        "        \"s_L_L_count\": 0,\n",
        "        \"s_H_H_count\": 0,\n",
        "        \"s_H_L_count\": 0,\n",
        "        \"s_L_a1_count\": 0,\n",
        "        \"s_H_a1_count\": 0\n",
        "    }\n",
        "    consecutive_errors = 0\n",
        "\n",
        "    for trial in range(NUM_SIMULATIONS):\n",
        "        state = \"theta_H\" if np.random.random() < PRIOR_P else \"theta_L\"\n",
        "        state_suffix = state.split(\"_\")[1]  # H or L\n",
        "\n",
        "        sender_prompt_text = sender_prompt(state, trial, history)\n",
        "        signal = call_llm(sender_prompt_text, role=\"sender\")\n",
        "        if signal not in SIGNALS:\n",
        "            signal = \"s_L\"\n",
        "            logging.warning(f\"无效信号（试验{trial+1}）：使用默认s_L\")\n",
        "            consecutive_errors += 1\n",
        "        else:\n",
        "            consecutive_errors = 0\n",
        "\n",
        "        if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:\n",
        "            logging.error(f\"连续错误超过{MAX_CONSECUTIVE_ERRORS}次，终止模拟\")\n",
        "            break\n",
        "\n",
        "        receiver_prompt_text = receiver_prompt(signal, trial, history)\n",
        "        action = call_llm(receiver_prompt_text, role=\"receiver\")\n",
        "        try:\n",
        "            action = int(action)\n",
        "            if action not in ACTIONS:\n",
        "                action = 0\n",
        "                logging.warning(f\"无效动作（试验{trial+1}）：使用默认0\")\n",
        "                consecutive_errors += 1\n",
        "            else:\n",
        "                consecutive_errors = 0\n",
        "        except (ValueError, TypeError):\n",
        "            action = 0\n",
        "            logging.warning(f\"动作解析错误（试验{trial+1}）：使用默认0\")\n",
        "            consecutive_errors += 1\n",
        "\n",
        "        if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:\n",
        "            logging.error(f\"连续错误超过{MAX_CONSECUTIVE_ERRORS}次，终止模拟\")\n",
        "            break\n",
        "\n",
        "        history[f\"{signal}_{state_suffix}_count\"] += 1\n",
        "        if action == 1:\n",
        "            history[f\"{signal}_a1_count\"] += 1\n",
        "\n",
        "        true_state_value = STATES[state]\n",
        "        receiver_payoff = 1 if action == 1 and true_state_value == 1 else -1 if action == 1 and true_state_value == 0 else 0\n",
        "        sender_payoff = action\n",
        "\n",
        "        results.append({\n",
        "            \"trial\": trial + 1,\n",
        "            \"state\": state,\n",
        "            \"signal\": signal,\n",
        "            \"action\": action,\n",
        "            \"sender_payoff\": sender_payoff,\n",
        "            \"receiver_payoff\": receiver_payoff\n",
        "        })\n",
        "\n",
        "        logging.info(f\"试验{trial+1} 历史: {history}\")\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    return results, history\n",
        "\n",
        "# 主函数\n",
        "def main():\n",
        "    logging.info(\"开始模拟交互实验...\")\n",
        "    try:\n",
        "        sim_results, history = run_simulation()\n",
        "\n",
        "        sender_avg_payoff = np.mean([r[\"sender_payoff\"] for r in sim_results]) if sim_results else 0\n",
        "        receiver_avg_payoff = np.mean([r[\"receiver_payoff\"] for r in sim_results]) if sim_results else 0\n",
        "\n",
        "        signal_counts = {\"theta_L\": {\"s_L\": 0, \"s_H\": 0}, \"theta_H\": {\"s_L\": 0, \"s_H\": 0}}\n",
        "        for r in sim_results:\n",
        "            signal_counts[r[\"state\"]][r[\"signal\"]] += 1\n",
        "        total_theta_L = signal_counts[\"theta_L\"][\"s_L\"] + signal_counts[\"theta_L\"][\"s_H\"]\n",
        "        total_theta_H = signal_counts[\"theta_H\"][\"s_L\"] + signal_counts[\"theta_H\"][\"s_H\"]\n",
        "        pi_s_L_theta_L = signal_counts[\"theta_L\"][\"s_L\"] / (total_theta_L + 1e-10)\n",
        "        pi_s_H_theta_H = signal_counts[\"theta_H\"][\"s_H\"] / (total_theta_H + 1e-10)\n",
        "\n",
        "        with open(\"simulation_results2.json\", \"w\") as f:\n",
        "            json.dump(sim_results, f, indent=2)\n",
        "\n",
        "        logging.info(f\"模拟完成：\")\n",
        "        logging.info(f\"发送者平均收益 = {sender_avg_payoff:.3f}\")\n",
        "        logging.info(f\"接收者平均收益 = {receiver_avg_payoff:.3f}\")\n",
        "        logging.info(f\"推断的发送者策略：\")\n",
        "        logging.info(f\"  π(s_L|theta_L) = {pi_s_L_theta_L:.3f}\")\n",
        "        logging.info(f\"  π(s_H|theta_H) = {pi_s_H_theta_H:.3f}\")\n",
        "        logging.info(f\"历史：{history}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"模拟失败：{str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"simulation_results2.json\")\n",
        "# Strategy convergence\n",
        "print(df.groupby(\"state\")[\"signal\"].value_counts(normalize=True))  # π(s|θ)\n",
        "print(df.groupby(\"signal\")[\"action\"].mean())  # P(a|s)\n",
        "# Payoffs\n",
        "print(f\"Sender avg payoff: {df['sender_payoff'].mean():.3f}\")\n",
        "print(f\"Receiver avg payoff: {df['receiver_payoff'].mean():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_70r-MxuKfz",
        "outputId": "e8a9875a-0776-403b-969f-6e75332a0f32"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state    signal\n",
            "theta_H  s_H       1.000000\n",
            "theta_L  s_L       0.617564\n",
            "         s_H       0.382436\n",
            "Name: proportion, dtype: float64\n",
            "signal\n",
            "s_H    0.925532\n",
            "s_L    0.000000\n",
            "Name: action, dtype: float64\n",
            "Sender avg payoff: 0.522\n",
            "Receiver avg payoff: 0.022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theta_L_trials = df[df[\"state\"] == \"theta_L\"]\n",
        "pi_sH_thetaL = (theta_L_trials[\"signal\"] == \"s_H\").cumsum() / theta_L_trials.index.map(lambda x: x + 1)\n",
        "print(pi_sH_thetaL)  # Should match plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTFqwICsvmN0",
        "outputId": "c36850a4-32c5-4790-85c4-3ef88bf38838"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1      0.000000\n",
            "2      0.000000\n",
            "3      0.000000\n",
            "4      0.200000\n",
            "5      0.166667\n",
            "         ...   \n",
            "993    0.271630\n",
            "995    0.271084\n",
            "997    0.270541\n",
            "998    0.270270\n",
            "999    0.270000\n",
            "Length: 706, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 读取 JSON 文件\n",
        "def read_simulation_results(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "        return []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"错误：无法解析 JSON 文件 {file_path}\")\n",
        "        return []\n",
        "\n",
        "# 计算并绘制所有策略概率\n",
        "def plot_sender_strategies(data):\n",
        "    trials_theta_H = []\n",
        "    trials_theta_L = []\n",
        "    pi_sL_thetaH = []\n",
        "    pi_sH_thetaH = []\n",
        "    pi_sH_thetaL = []\n",
        "    pi_sL_thetaL = []\n",
        "    theta_H_count = 0\n",
        "    theta_L_count = 0\n",
        "    sL_thetaH_count = 0\n",
        "    sH_thetaH_count = 0\n",
        "    sH_thetaL_count = 0\n",
        "    sL_thetaL_count = 0\n",
        "\n",
        "    for trial in data:\n",
        "        state = trial[\"state\"]\n",
        "        signal = trial[\"signal\"]\n",
        "\n",
        "        if state == \"theta_H\":\n",
        "            theta_H_count += 1\n",
        "            if signal == \"s_L\":\n",
        "                sL_thetaH_count += 1\n",
        "            elif signal == \"s_H\":\n",
        "                sH_thetaH_count += 1\n",
        "            trials_theta_H.append(trial[\"trial\"])\n",
        "            pi_sL_thetaH.append(sL_thetaH_count / theta_H_count if theta_H_count > 0 else 0)\n",
        "            pi_sH_thetaH.append(sH_thetaH_count / theta_H_count if theta_H_count > 0 else 0)\n",
        "\n",
        "        elif state == \"theta_L\":\n",
        "            theta_L_count += 1\n",
        "            if signal == \"s_H\":\n",
        "                sH_thetaL_count += 1\n",
        "            elif signal == \"s_L\":\n",
        "                sL_thetaL_count += 1\n",
        "            trials_theta_L.append(trial[\"trial\"])\n",
        "            pi_sH_thetaL.append(sH_thetaL_count / theta_L_count if theta_L_count > 0 else 0)\n",
        "            pi_sL_thetaL.append(sL_thetaL_count / theta_L_count if theta_L_count > 0 else 0)\n",
        "\n",
        "    # 绘制图表\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    if trials_theta_H:\n",
        "        plt.plot(trials_theta_H, pi_sL_thetaH, 'b-', label=r'$\\pi(s_L|\\theta_H)$')\n",
        "        plt.plot(trials_theta_H, pi_sH_thetaH, 'r-', label=r'$\\pi(s_H|\\theta_H)$')\n",
        "\n",
        "    if trials_theta_L:\n",
        "        plt.plot(trials_theta_L, pi_sH_thetaL, 'g-', label=r'$\\pi(s_H|\\theta_L)$')\n",
        "        plt.plot(trials_theta_L, pi_sL_thetaL, 'm-', label=r'$\\pi(s_L|\\theta_L)$')\n",
        "\n",
        "    if not (trials_theta_H or trials_theta_L):\n",
        "        print(\"没有 theta_H 或 theta_L 试验，无法绘制图表\")\n",
        "        return\n",
        "\n",
        "    plt.xlabel('Trial Number')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title('Evolution of Sender Strategies: Conditional Signal Probabilities')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('sender_strategies.png')\n",
        "    plt.close()\n",
        "    print(\"图表已保存为 sender_strategies.png\")\n",
        "\n",
        "# 主函数\n",
        "def main():\n",
        "    file_path = \"simulation_results2.json\"\n",
        "    data = read_simulation_results(file_path)\n",
        "    if data:\n",
        "        plot_sender_strategies(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1P_55nzQstr",
        "outputId": "299ac046-f6a4-447f-fe10-a6372548ad47"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "图表已保存为 sender_strategies.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 计算并绘制 P(theta|s)\n",
        "def plot_posterior_beliefs(data):\n",
        "    trials_sH = []\n",
        "    trials_sL = []\n",
        "    p_thetaH_sH = []\n",
        "    p_thetaL_sH = []\n",
        "    p_thetaH_sL = []\n",
        "    p_thetaL_sL = []\n",
        "    sH_count = 0\n",
        "    sL_count = 0\n",
        "    thetaH_sH_count = 0\n",
        "    thetaL_sH_count = 0\n",
        "    thetaH_sL_count = 0\n",
        "    thetaL_sL_count = 0\n",
        "\n",
        "    for trial in data:\n",
        "        state = trial[\"state\"]\n",
        "        signal = trial[\"signal\"]\n",
        "\n",
        "        if signal == \"s_H\":\n",
        "            sH_count += 1\n",
        "            if state == \"theta_H\":\n",
        "                thetaH_sH_count += 1\n",
        "            elif state == \"theta_L\":\n",
        "                thetaL_sH_count += 1\n",
        "            trials_sH.append(trial[\"trial\"])\n",
        "            p_thetaH_sH.append(thetaH_sH_count / sH_count if sH_count > 0 else 0)\n",
        "            p_thetaL_sH.append(thetaL_sH_count / sH_count if sH_count > 0 else 0)\n",
        "\n",
        "        elif signal == \"s_L\":\n",
        "            sL_count += 1\n",
        "            if state == \"theta_H\":\n",
        "                thetaH_sL_count += 1\n",
        "            elif state == \"theta_L\":\n",
        "                thetaL_sL_count += 1\n",
        "            trials_sL.append(trial[\"trial\"])\n",
        "            p_thetaH_sL.append(thetaH_sL_count / sL_count if sL_count > 0 else 0)\n",
        "            p_thetaL_sL.append(thetaL_sL_count / sL_count if sL_count > 0 else 0)\n",
        "\n",
        "    # 绘制图表\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    if trials_sH:\n",
        "        plt.plot(trials_sH, p_thetaH_sH, 'b-', label=r'$P(\\theta_H|s_H)$')\n",
        "        plt.plot(trials_sH, p_thetaL_sH, 'r-', label=r'$P(\\theta_L|s_H)$')\n",
        "\n",
        "    if trials_sL:\n",
        "        plt.plot(trials_sL, p_thetaH_sL, 'g-', label=r'$P(\\theta_H|s_L)$')\n",
        "        plt.plot(trials_sL, p_thetaL_sL, 'm-', label=r'$P(\\theta_L|s_L)$')\n",
        "\n",
        "    if not (trials_sH or trials_sL):\n",
        "        print(\"没有 s_H 或 s_L 试验，无法绘制图表\")\n",
        "        return\n",
        "\n",
        "    plt.xlabel('Trial Number')\n",
        "    plt.ylabel('Posterior Probability')\n",
        "    plt.title('Evolution of Receiver Beliefs: Posterior Probabilities $P(\\theta|s)$')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('posterior_beliefs.png')\n",
        "    plt.close()\n",
        "    print(\"图表已保存为 posterior_beliefs.png\")\n",
        "\n",
        "# 主函数\n",
        "def main():\n",
        "    file_path = \"simulation_results2.json\"\n",
        "    data = read_simulation_results(file_path)\n",
        "    if data:\n",
        "        plot_posterior_beliefs(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_SutsgtRlZp",
        "outputId": "c619f271-a348-4fda-954f-7cac680c77e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "图表已保存为 posterior_beliefs.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 计算并绘制累计平均收益\n",
        "def plot_payoffs(data):\n",
        "    trials = []\n",
        "    sender_payoffs = []\n",
        "    receiver_payoffs = []\n",
        "    sender_cumsum = 0\n",
        "    receiver_cumsum = 0\n",
        "\n",
        "    for trial in data:\n",
        "        trials.append(trial[\"trial\"])\n",
        "        sender_cumsum += trial[\"sender_payoff\"]\n",
        "        receiver_cumsum += trial[\"receiver_payoff\"]\n",
        "        sender_payoffs.append(sender_cumsum / trial[\"trial\"])\n",
        "        receiver_payoffs.append(receiver_cumsum / trial[\"trial\"])\n",
        "\n",
        "    if not trials:\n",
        "        print(\"没有试验数据，无法绘制图表\")\n",
        "        return\n",
        "\n",
        "    # 绘制图表\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # 累计平均收益\n",
        "    plt.plot(trials, sender_payoffs, 'b-', label='Sender Cumulative Average Payoff')\n",
        "    plt.plot(trials, receiver_payoffs, 'r-', label='Receiver Cumulative Average Payoff')\n",
        "\n",
        "    # 理论最优收益\n",
        "    plt.axhline(y=0.6, color='g', linestyle='--', label='Sender Theoretical Optimal Payoff (0.6)')\n",
        "    plt.axhline(y=0, color='m', linestyle='--', label='Receiver Theoretical Optimal Payoff (0)')\n",
        "\n",
        "    # 完全信息收益\n",
        "    plt.axhline(y=0.3, color='c', linestyle=':', label='Sender Full Information Payoff (0.3)')\n",
        "    plt.axhline(y=0.3, color='y', linestyle=':', label='Receiver Full Information Payoff (0.3)')\n",
        "\n",
        "    plt.xlabel('Trial Number')\n",
        "    plt.ylabel('Cumulative Average Payoff')\n",
        "    plt.title('Sender and Receiver Payoffs with Theoretical and Full Information Benchmarks')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('payoffs.png')\n",
        "    plt.close()\n",
        "    print(\"图表已保存为 payoffs.png\")\n",
        "\n",
        "# 主函数\n",
        "def main():\n",
        "    file_path = \"simulation_results2.json\"\n",
        "    data = read_simulation_results(file_path)\n",
        "    if data:\n",
        "        plot_payoffs(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH8yFjmkSoMd",
        "outputId": "ca6ddc8e-90f0-429e-e2e4-579f24cc81e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "图表已保存为 payoffs.png\n"
          ]
        }
      ]
    }
  ]
}